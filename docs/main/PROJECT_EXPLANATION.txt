================================================================================
ECE 432 PROJECT: ML-ASSISTED HAMMING CODE DECODER
================================================================================

Authors: Shashwat Sinha, Ambarish Pathak
Course: ECE 432
Date: December 2025

================================================================================
PROJECT OVERVIEW
================================================================================

This project implements and evaluates machine learning-assisted decoders for
Hamming(7,4) error-correcting codes. The primary objective is to determine
whether neural networks can improve upon classical syndrome-based decoding
performance over Additive White Gaussian Noise (AWGN) channels.

The Hamming(7,4) code is a linear error-correcting code that encodes 4 data bits
into 7 codeword bits, enabling detection and correction of single-bit errors.
This project demonstrates that ML-assisted decoders can significantly outperform
classical decoders by learning complex error patterns that traditional methods
might miss.

The key innovation is the use of soft-decision inputs (Log-Likelihood Ratios)
rather than hard-decision bits, which provides the neural network with
reliability information from the channel. This approach achieves an average
43-45% improvement in bit error rate compared to classical decoding.

================================================================================
HOW THE PROJECT WORKS
================================================================================

1. ENCODING AND TRANSMISSION
----------------------------

The system follows a complete digital communication pipeline:

a) Data Encoding:
   - 4-bit data messages are encoded into 7-bit Hamming codewords
   - Uses systematic generator matrix with parity check equations
   - All 16 possible 4-bit messages can be encoded

b) Modulation:
   - Binary Phase Shift Keying (BPSK) modulation
   - Binary 0 maps to +1, binary 1 maps to -1
   - Standard modulation scheme for AWGN channels

c) Channel Transmission:
   - Additive White Gaussian Noise (AWGN) channel
   - Noise variance calculated from Eb/N0 (energy per bit to noise ratio)
   - Eb/N0 range: -5 dB to 10 dB (typical communication range)

d) Demodulation:
   - Two approaches: hard-decision and soft-decision
   - Hard-decision: threshold at 0 (received > 0 → 0, received < 0 → 1)
   - Soft-decision: computes Log-Likelihood Ratios (LLRs)
     * LLR = 2 × received_symbol / noise_variance
     * Positive LLR → likely 0, negative LLR → likely 1
     * Magnitude indicates confidence level

2. DECODING APPROACHES
----------------------

a) Classical Decoder (Baseline):
   - Syndrome-based decoding using lookup table
   - Detects and corrects single-bit errors
   - Standard approach for Hamming codes
   - Provides baseline performance for comparison

b) ML-Assisted Decoders:
   - Neural network architectures that learn error correction patterns
   - Multiple architecture variants explored (standard, deep, wide, residual)
   - Trained on synthetic data generated from channel simulations
   - Use soft-decision inputs (Log-Likelihood Ratios) for optimal performance

3. NEURAL NETWORK ARCHITECTURES
-------------------------------

The project implements several neural network architectures:

a) Standard Architecture:
   - Input: 7 received bits (or LLRs)
   - Hidden layers: 64 → 32 neurons
   - Output: 4 data bits
   - Fully connected layers with ReLU activation
   - Dropout (0.2) for regularization
   - Sigmoid output for binary classification

b) Deep Architecture:
   - Input: 7 received bits (LLRs)
   - Architecture: 7 → 128 → 64 → 32 → 16 → 4
   - 5 layers for hierarchical feature learning
   - Better capacity for complex error patterns

c) Wide Architecture:
   - Input: 7 received bits (LLRs)
   - Architecture: 7 → 256 → 128 → 4
   - Wider layers for increased capacity
   - Better for learning complex simultaneous patterns

d) Residual Architecture:
   - Input: 7 received bits (LLRs)
   - Architecture: 7 → 128 → 64 → 32 → 4 with skip connections
   - Residual blocks help with gradient flow
   - Enables learning identity mappings
   - Best overall performance (44.5% improvement)

4. TRAINING PROCESS
-------------------

a) Training Data Generation:
   - Synthetic data generated by simulating transmission
   - For each sample:
     * Random 4-bit data → Encode → Modulate → Add noise → Demodulate
     * Input: received bits/LLRs
     * Target: original data bits
   - Weighted sampling: 70% of training samples from error-prone region (0-5 dB)
   - Data augmentation: 10% of samples augmented with controlled noise

b) Training Configuration:
   - Loss function: Binary Cross-Entropy (BCE) or Codeword-level loss
   - Codeword-level loss: Penalizes incorrect codewords more than individual bits
   - Optimizer: Adam (learning rate = 0.001)
   - Batch size: 64
   - Epochs: 50
   - Validation split: 80% train, 20% validation

5. EVALUATION METHODOLOGY
-------------------------

a) Performance Metric:
   - Bit Error Rate (BER) = (number of bit errors) / (total bits transmitted)
   - Evaluated across Eb/N0 range: -5 to 10 dB
   - 16 evaluation points
   - 100,000 bits per point for statistical significance

b) Comparison Framework:
   - Classical decoder provides baseline
   - All ML variants evaluated under identical conditions
   - Improvement calculated as: ((BER_classical - BER_ML) / BER_classical) × 100%

================================================================================
WHAT THE PROJECT ACHIEVES
================================================================================

1. PERFORMANCE IMPROVEMENTS
---------------------------

The project successfully demonstrates that ML-assisted decoders can significantly
outperform classical decoders:

a) Soft-Decision ML Decoder (Primary Approach):
   - Performance: 43.3% BETTER than classical on average
   - Best improvement: 100% (zero errors) at 8-10 dB where classical has errors
   - Medium Eb/N0 (0-5 dB): 25-67% improvement
   - Critical innovation: Using Log-Likelihood Ratios (LLRs) instead of hard bits

b) Improved Training Strategy:
   - Performance: 43.7% better than classical
   - Weighted sampling focuses training on error-prone regions
   - Codeword-level loss function improves learning
   - Data augmentation increases robustness

c) Architecture Variants:
   - Deep Architecture: 43.0% better than classical
   - Wide Architecture: 43.9% better than classical
   - Residual Architecture: 44.5% better than classical (BEST)
   - Different architectures excel in different Eb/N0 regions

2. KEY ACHIEVEMENTS
-------------------

a) Successful ML Integration:
   - Demonstrated that neural networks can learn error correction patterns
   - Achieved consistent improvements across Eb/N0 range
   - Validated approach through comprehensive evaluation

b) Soft-Decision Innovation:
   - Identified that hard-decision inputs were limiting performance
   - Implemented soft-decision using Log-Likelihood Ratios (LLRs)
   - Provides reliability information from channel to neural network
   - Enables model to make better error correction decisions

c) Training Strategy Optimization:
   - Weighted sampling improves learning in error-prone regions
   - Codeword-level loss aligns with Hamming code structure
   - Data augmentation improves generalization

d) Architecture Exploration:
   - Evaluated multiple neural network architectures
   - Identified residual architecture as best performer
   - Demonstrated architecture selection impacts performance

e) Comprehensive Evaluation:
   - Evaluated all model variants under identical conditions
   - Generated comprehensive comparison plots
   - Created detailed performance reports

================================================================================
KEY FINDINGS
================================================================================

1. SOFT-DECISION INPUTS ARE CRITICAL
------------------------------------

The most important finding is that soft-decision inputs (Log-Likelihood Ratios)
are essential for ML decoder performance:

- Hard-decision ML: Performs worse than classical decoder
- Soft-decision ML: 43.3% better than classical on average
- Difference: Transformation from underperforming to significantly outperforming

Why LLRs work:
- Provide reliability information (confidence in each bit)
- Continuous values contain more information than binary 0/1
- Model can learn to weight uncertain bits differently
- Enables better error correction decisions
- LLR magnitude indicates confidence level

2. PERFORMANCE BY EB/N0 REGION
-------------------------------

a) Low Eb/N0 (-5 to -1 dB):
   - High noise, many errors
   - Soft-decision ML: 11-22% better than classical
   - Improvement is modest but consistent

b) Medium Eb/N0 (0 to 5 dB) - CRITICAL REGION:
   - Error-prone region where most practical systems operate
   - Soft-decision ML: 25-67% better than classical
   - Maximum improvement at 5 dB: 67% better
   - This is where ML decoder provides most value

c) High Eb/N0 (6 to 10 dB):
   - Low noise, few errors
   - Soft-decision ML: Achieves zero errors at 8-10 dB
   - Classical decoder sometimes has errors even at high Eb/N0
   - ML decoder achieves perfect decoding

3. TRAINING STRATEGY MATTERS
------------------------------

Training strategy significantly impacts performance:

a) Weighted Sampling:
   - Focusing 70% of training on error-prone region (0-5 dB) improves learning
   - Model sees more error patterns where they actually occur
   - Better generalization to practical operating conditions

b) Codeword-Level Loss:
   - Encourages model to learn complete codeword structure
   - Penalizes incorrect codewords more than individual bit errors
   - Better alignment with Hamming code properties
   - Combines bit-level and codeword-level penalties

c) Data Augmentation:
   - Increases training data diversity
   - Improves model robustness
   - Better generalization to unseen patterns
   - Adds controlled noise to 10% of training samples

4. ARCHITECTURE SELECTION
--------------------------

Exploration of different architectures revealed distinct characteristics:

a) Deep Architecture:
   - Better for hierarchical error patterns
   - More layers enable multi-level reasoning
   - Slightly lower performance (43.0%) but good for complex patterns

b) Wide Architecture:
   - Better for complex simultaneous patterns
   - More neurons per layer provide increased capacity
   - Good performance (43.9%) with faster training

c) Residual Architecture:
   - Best overall performance (44.5%)
   - Skip connections help with gradient flow
   - Better training stability
   - Recommended for practical use

5. PRACTICAL SIGNIFICANCE
-------------------------

The improvements are substantial and consistent:

- Average 43-45% improvement over classical decoder
- Best improvement: 100% (zero errors) at high Eb/N0
- Consistent improvements across entire Eb/N0 range
- Most significant gains in error-prone region (0-5 dB) where practical
  systems operate

================================================================================
TECHNICAL DETAILS
================================================================================

1. HAMMING(7,4) CODE
--------------------

- Code Rate: R = 4/7 (4 data bits → 7 codeword bits)
- Error Correction: Single-bit errors
- Minimum Distance: d_min = 3
- Generator Matrix: Systematic form with parity equations
- Syndrome Decoding: Standard lookup table approach

2. CHANNEL MODEL
----------------

- Modulation: BPSK (0 → +1, 1 → -1)
- Channel: AWGN (Additive White Gaussian Noise)
- Noise: n ~ N(0, σ²) where σ² = 1/(2 × R × 10^(Eb/N0/10))
- Eb/N0 Range: -5 to 10 dB (evaluated at 16 points)

3. ML ARCHITECTURE DETAILS
---------------------------

- Framework: PyTorch
- Input: 7 received bits (hard) or LLRs (soft)
- Hidden Layers: Fully connected with ReLU activation
- Output: 4 data bits (Sigmoid activation, threshold at 0.5)
- Regularization: Dropout (0.2)
- Training: Adam optimizer, learning rate 0.001

4. EVALUATION SETUP
-------------------

- Eb/N0 Range: -5 to 10 dB
- Number of Points: 16
- Bits per Point: 100,000
- Random Seed: 42 (for reproducibility)
- All models evaluated under identical conditions

================================================================================
PROJECT STRUCTURE
================================================================================

Core Source Code (src/):
- hamming.py: Hamming(7,4) encoder/decoder
- classical_decoder.py: Syndrome-based decoder
- channel.py: BPSK modulation and AWGN channel
- evaluation.py: BER evaluation framework
- ml_decoder.py: Neural network architectures
- ml_evaluation.py: ML decoder evaluation

Training Scripts:
- train_ml_decoder.py: Hard-decision training
- train_ml_decoder_soft.py: Soft-decision training (recommended)
- train_ml_decoder_phase3.py: Improved training with weighted sampling
- train_ml_decoder_phase4.py: Architecture variants (deep, wide, residual)

Evaluation Scripts:
- run_baseline_evaluation.py: Classical decoder evaluation
- compare_decoders.py: Two-model comparison
- final_evaluation_phase5.py: Comprehensive evaluation (all models)

Verification Scripts:
- verify_phase1.py through verify_phase6.py: Component verification

Tests:
- 66 unit tests covering all components
- Comprehensive test coverage

Documentation:
- README.md: Complete project documentation
- PROJECT_SUMMARY.md: Project summary
- docs/CODEBASE_DOCUMENTATION.md: Detailed technical documentation
- docs/SOFT_DECISION_ANALYSIS.md: Soft-decision analysis
- docs/PHASE3_IMPROVEMENTS.md: Training strategy details
- docs/PHASE4_ARCHITECTURES.md: Architecture details
- docs/PHASE5_FINAL_EVALUATION.md: Final evaluation details

Results:
- data/phase5_final_comparison.png: Comprehensive comparison plot (all models)
- data/phase5_final_report.md: Performance summary report
- data/phase5_final_results.npy: Raw results data

================================================================================
CONCLUSIONS
================================================================================

1. SUCCESSFUL DEMONSTRATION
---------------------------

The project successfully demonstrates that ML-assisted decoders can significantly
improve upon classical decoding performance. The soft-decision ML decoder achieves
an average 43-45% improvement over the classical decoder, with the best
performance reaching 100% improvement (zero errors) at high Eb/N0 values.

2. KEY INSIGHTS
---------------

a) Soft-decision inputs (LLRs) are essential for ML decoder performance
b) Training strategy significantly impacts model performance
c) Architecture selection matters, with residual architecture performing best
d) Most significant improvements occur in error-prone regions (0-5 dB)

3. PRACTICAL IMPLICATIONS
--------------------------

The results have practical significance for digital communication systems:
- ML-assisted decoders can improve error correction performance
- Improvements are most significant where errors are common (0-5 dB)
- Soft-decision approach is critical for success
- Residual architecture recommended for practical use

4. FUTURE WORK
--------------

Potential future directions:
- Extend to other error-correcting codes
- Explore more advanced architectures (transformers, attention)
- Investigate ensemble methods
- Hardware implementation (FPGA/ASIC)
- Real-world channel testing

================================================================================
REPRODUCIBILITY
================================================================================

All results are reproducible:
- Fixed random seeds (default: 42)
- Deterministic evaluation
- Saved model checkpoints
- Complete documentation
- All code available

To reproduce results:
1. Use same random seed
2. Use same number of bits per evaluation point
3. Use same Eb/N0 range and number of points
4. Load saved models from models/ directory

================================================================================
END OF DOCUMENT
================================================================================

