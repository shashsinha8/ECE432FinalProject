# ECE 432 Project: ML-Assisted Hamming Code Decoder
## Comprehensive Codebase Documentation

**Authors**: Shashwat Sinha, Ambarish Pathak  
**Course**: ECE 432  
**Date**: December 2025

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Initial Development: The Six Phases](#initial-development-the-six-phases)
3. [Discovery of Performance Issues](#discovery-of-performance-issues)
4. [Analysis and Root Cause Identification](#analysis-and-root-cause-identification)
5. [Improvement Phases](#improvement-phases)
6. [Pending Improvements](#pending-improvements)
7. [Codebase Architecture](#codebase-architecture)
8. [Key Technical Details](#key-technical-details)

---

## Project Overview

This project implements a machine learning-assisted decoder for Hamming(7,4) error-correcting codes. The Hamming(7,4) code encodes 4 data bits into 7 codeword bits, enabling detection and correction of single-bit errors. The project explores whether neural networks can improve upon classical syndrome-based decoding performance over Additive White Gaussian Noise (AWGN) channels.

### Core Objective

The primary goal is to develop an ML decoder that outperforms the classical decoder by learning complex error patterns that the classical decoder might miss, particularly in challenging channel conditions.

### Project Structure

```
ECE432FinalProject/
â”œâ”€â”€ src/                          # Core source code modules
â”‚   â”œâ”€â”€ hamming.py                # Hamming(7,4) encoder/decoder
â”‚   â”œâ”€â”€ classical_decoder.py      # Classical syndrome-based decoder
â”‚   â”œâ”€â”€ channel.py                # BPSK modulation & AWGN channel
â”‚   â”œâ”€â”€ evaluation.py             # BER evaluation framework
â”‚   â”œâ”€â”€ ml_decoder.py             # ML model architectures
â”‚   â””â”€â”€ ml_evaluation.py          # ML decoder evaluation
â”œâ”€â”€ tests/                        # Comprehensive test suite (66 tests)
â”œâ”€â”€ data/                         # Results, plots, and analysis
â”œâ”€â”€ models/                       # Saved trained ML models
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ train_ml_decoder.py           # Hard-decision ML training
â”œâ”€â”€ train_ml_decoder_soft.py     # Soft-decision ML training
â”œâ”€â”€ compare_decoders.py           # Comparison script
â”œâ”€â”€ analyze_results.py            # Performance analysis
â””â”€â”€ verify_phase*.py              # Phase verification scripts
```

---

## Initial Development: The Six Phases

The project was initially developed in six sequential phases, each building upon the previous one. This section documents how the codebase was constructed from the ground up.

### Phase 1: Foundation & Classical Decoder âœ“

**Objective**: Implement the fundamental Hamming(7,4) code encoder and classical decoder.

**Implementation Details**:

1. **Hamming Encoder** (`src/hamming.py`):
   - Implemented generator matrix for systematic Hamming(7,4) encoding
   - Encodes 4 data bits into 7 codeword bits
   - Uses parity check equations: `p1 = d1 âŠ• d2 âŠ• d4`, `p2 = d1 âŠ• d3 âŠ• d4`, `p3 = d2 âŠ• d3 âŠ• d4`
   - All 16 possible 4-bit messages encode/decode correctly

2. **Classical Decoder** (`src/classical_decoder.py`):
   - Syndrome-based decoding using lookup table
   - Detects and corrects single-bit errors
   - Returns decoded data bits, corrected codeword, and error information

**Key Code Structure**:
```python
# src/hamming.py
def encode(data_bits):
    """Encode 4 data bits into 7-bit Hamming codeword"""
    # Generator matrix multiplication
    # Returns 7-bit codeword

# src/classical_decoder.py
class ClassicalDecoder:
    def decode(self, received_bits):
        """Decode using syndrome lookup"""
        # Calculate syndrome
        # Lookup error pattern
        # Correct and extract data bits
```

**Verification**: All 15 unit tests passing, verified all 16 possible messages encode/decode correctly.

---

### Phase 2: Channel Simulation âœ“

**Objective**: Implement realistic communication channel with BPSK modulation and AWGN noise.

**Implementation Details**:

1. **BPSK Modulation** (`src/channel.py`):
   - Maps binary 0 â†’ +1, binary 1 â†’ -1
   - Standard BPSK modulation for AWGN channels

2. **AWGN Channel**:
   - Adds Gaussian noise with variance calculated from Eb/N0
   - Proper conversion: `ÏƒÂ² = Nâ‚€/2 = 1/(2 Ã— R Ã— 10^(Eb/N0/10))`
   - Where R = 4/7 (code rate)

3. **Demodulation**:
   - **Hard-decision**: Threshold at 0 (received > 0 â†’ 0, received < 0 â†’ 1)
   - **Soft-decision**: Computes Log-Likelihood Ratios (LLRs)
     - `LLR = 2 Ã— received_symbol / ÏƒÂ²`
     - Positive LLR â†’ likely 0, negative LLR â†’ likely 1
     - Magnitude indicates confidence

**Key Code Structure**:
```python
# src/channel.py
def bpsk_modulate(bits):
    """Convert bits to BPSK symbols: 0â†’+1, 1â†’-1"""
    
def awgn_channel(symbols, ebno_db, rate, seed=None):
    """Add AWGN noise with proper variance"""
    
def bpsk_demodulate_hard(noisy_symbols):
    """Hard-decision: threshold at 0"""
    
def bpsk_demodulate_soft(noisy_symbols, noise_variance):
    """Soft-decision: compute LLRs"""
```

**Verification**: 24 unit tests passing, noise statistics verified to match expected values.

---

### Phase 3: Baseline Performance Evaluation âœ“

**Objective**: Establish baseline performance of classical decoder across Eb/N0 range.

**Implementation Details**:

1. **BER Calculation Framework** (`src/evaluation.py`):
   - Bit Error Rate (BER) = (number of bit errors) / (total bits transmitted)
   - Evaluates decoder across range of Eb/N0 values (typically -5 to 10 dB)
   - Uses fixed random seed for reproducibility

2. **Evaluation Pipeline**:
   - Generate random data bits
   - Encode â†’ Modulate â†’ Add noise â†’ Demodulate â†’ Decode
   - Count errors and calculate BER
   - Repeat for multiple Eb/N0 points

3. **Visualization**:
   - BER vs. Eb/N0 curves (log scale)
   - Saves results to `data/baseline_ber_curve.png`

**Key Code Structure**:
```python
# src/evaluation.py
def calculate_ber(decoder, ebno_db, num_bits, seed=None):
    """Calculate BER for given Eb/N0"""
    
def evaluate_decoder(decoder, ebno_range, num_bits_per_point, seed=None):
    """Evaluate decoder across Eb/N0 range"""
    
def plot_ber_curve(ebno_range, ber_values, title, save_path):
    """Plot BER vs. Eb/N0 curve"""
```

**Verification**: 11 unit tests passing, baseline performance matches expected Hamming code behavior.

**Results**: Classical decoder shows expected performance:
- High BER at low Eb/N0 (e.g., ~0.3 at -5 dB)
- Decreasing BER as Eb/N0 increases
- Very low BER at high Eb/N0 (approaching zero errors)

---

### Phase 4: ML Model Development âœ“

**Objective**: Design and implement neural network architectures for ML-assisted decoding.

**Implementation Details**:

1. **Two Architectures** (`src/ml_decoder.py`):

   **a) DirectMappingDecoder**:
   - Input: 7 received bits (hard decisions)
   - Output: 4 data bits
   - Architecture: 7 â†’ 64 â†’ 32 â†’ 4 (fully connected)
   - Activation: ReLU in hidden layers, Sigmoid at output
   - Dropout: 0.2 for regularization

   **b) PostProcessingDecoder**:
   - Input: 7 bits from classical decoder output
   - Output: 4 corrected data bits
   - Architecture: 7 â†’ 32 â†’ 16 â†’ 4
   - Idea: Let classical decoder do initial correction, ML fixes remaining errors

2. **Training Data Generation**:
   - Generates synthetic training data by simulating transmission
   - For each sample:
     - Random 4-bit data â†’ Encode â†’ Modulate â†’ Add noise â†’ Demodulate
     - Input: received bits (or classical decoder output)
     - Target: original data bits
   - Uniform sampling across Eb/N0 range (-5 to 10 dB)

3. **Training Framework**:
   - PyTorch implementation
   - Loss function: Binary Cross-Entropy (BCE)
   - Optimizer: Adam (learning rate = 0.001)
   - Validation split: 80% train, 20% validation
   - Metrics: Loss and accuracy (codeword-level)

**Key Code Structure**:
```python
# src/ml_decoder.py
class DirectMappingDecoder(nn.Module):
    """Neural network: 7 received bits â†’ 4 data bits"""
    def __init__(self, input_size=7, hidden_sizes=[64, 32], output_size=4):
        # Build fully connected layers
        
def generate_training_data(num_samples, ebno_range, approach='direct', use_soft_input=False):
    """Generate training data by simulating transmission"""
    # For each sample:
    #   - Generate random data
    #   - Encode, modulate, add noise, demodulate
    #   - Return (input, target) pair
    
def train_model(model, train_loader, val_loader, num_epochs=50):
    """Train model with validation"""
```

**Training Process**:
- Training script: `train_ml_decoder.py`
- Default: 100,000 samples, 50 epochs
- Saves model to `models/ml_decoder_direct.pth`

**Verification**: 11 unit tests passing, models train successfully, inference works correctly.

**Initial Results**:
- Direct mapping approach showed promise
- Validation accuracy: ~78% (hard-decision model)
- Model learned to map received bits to data bits

---

### Phase 5: ML-Assisted Decoding Integration âœ“

**Objective**: Integrate ML decoder into evaluation pipeline and enable comparison.

**Implementation Details**:

1. **ML Decoder Wrapper** (`src/ml_decoder.py`):
   - `MLDecoder` class wraps trained PyTorch model
   - Provides `decode()` method compatible with evaluation framework
   - Handles batch processing and device management

2. **ML Evaluation Framework** (`src/ml_evaluation.py`):
   - Similar to classical evaluation but uses ML decoder
   - Loads trained model from file
   - Evaluates across Eb/N0 range
   - Saves results for comparison

3. **Comparison Functionality**:
   - `compare_decoders.py`: Side-by-side comparison
   - Generates comparison plots
   - Calculates performance differences

**Key Code Structure**:
```python
# src/ml_decoder.py
class MLDecoder:
    """Wrapper for trained ML model"""
    def decode(self, received_bits):
        """Decode using trained model"""
        # Convert to tensor
        # Run inference
        # Threshold at 0.5
        # Return decoded bits

# src/ml_evaluation.py
def run_ml_evaluation(model_path, approach, ebno_range, num_bits, seed=None):
    """Evaluate ML decoder across Eb/N0 range"""
```

**Verification**: 5 unit tests passing, ML decoder integrates seamlessly with evaluation.

---

### Phase 6: Final Comparison & Documentation âœ“

**Objective**: Generate final results, create comprehensive documentation, and ensure reproducibility.

**Implementation Details**:

1. **Final Results Generation** (`generate_final_results.py`):
   - Runs both classical and ML evaluations
   - Generates comprehensive comparison plots
   - Saves results to `data/` directory

2. **Documentation**:
   - Comprehensive README with setup instructions
   - Example usage scripts (`example_usage.py`)
   - Project summary document
   - Code documentation and docstrings

3. **Reproducibility**:
   - Fixed random seeds (default: 42)
   - Deterministic results with same parameters
   - Saved model checkpoints

**Verification**: All documentation and scripts verified, results reproducible.

---

## Discovery of Performance Issues

After completing all six phases, the project was functionally complete. However, when we ran comprehensive evaluations comparing the ML decoder to the classical decoder, we discovered **significant performance problems**.

### Initial Expectations vs. Reality

**Expected**: ML decoder should outperform classical decoder by learning complex error patterns.

**Reality**: ML decoder performed **worse** than classical decoder, particularly at medium and high Eb/N0 values.

### Initial Observations

When running `compare_decoders.py` and `generate_final_results.py`, we noticed:

1. **At low Eb/N0** (-5 to -1 dB): ML decoder performed slightly better (0-3% improvement)
2. **At medium Eb/N0** (0 to 5 dB): ML decoder performed **significantly worse** (6-101% degradation)
3. **At high Eb/N0** (6-10 dB): ML decoder had errors even when classical decoder had zero errors

### Key Metrics from Initial Results

- **Average performance**: ML decoder was **116.7% worse** than classical decoder
- **Medium Eb/N0 region**: Average **38.1% degradation**
- **High Eb/N0 region**: Average **468.7% degradation**
- **Critical issue**: At Eb/N0 = 8 dB, classical decoder had zero errors, but ML decoder had BER = 1.21Ã—10â»Â³

This was a major problem. The ML decoder was not only failing to improve upon classical decoding, but was actually making things worse.

---

## Analysis and Root Cause Identification

To understand why the ML decoder was underperforming, we created a comprehensive analysis script (`analyze_results.py`) that performed detailed performance analysis and identified root causes.

### Phase 1 (Improvement): Analysis & Visualization âœ“

**Objective**: Thoroughly analyze performance, identify issues, and generate visualizations.

**Implementation** (`analyze_results.py`):

1. **Performance Metrics Calculation**:
   - Calculated improvement/degradation percentage at each Eb/N0 point
   - Analyzed performance by region (low, medium, high Eb/N0)
   - Identified best and worst performance points
   - Calculated average improvements across regions

2. **Visualization**:
   - Annotated comparison plots showing performance differences
   - Highlighted problem regions
   - Generated `data/comparison_analysis.png`

3. **Analysis Report**:
   - Generated markdown report (`data/analysis_report.md`)
   - Documented all findings and recommendations

**Key Findings**:

#### Issue 1: Hard-Decision Input Limitation

**Problem**: The ML decoder was receiving only hard-decision bits (0 or 1), losing all reliability information from the channel.

**Root Cause**:
- Classical decoder is optimized for hard-decision decoding
- ML decoder was trained on hard bits, which contain no information about channel confidence
- At medium Eb/N0, many bits are uncertain, but ML decoder only sees binary values

**Impact**: 
- ML decoder cannot distinguish between confident and uncertain bits
- Cannot leverage soft reliability information that could improve decoding

**Evidence**:
- Performance degradation was worst at medium Eb/N0 (0-5 dB) where uncertainty is highest
- At high Eb/N0, classical decoder achieves near-zero errors, but ML decoder still makes mistakes

#### Issue 2: Training Data Distribution

**Problem**: Training data was uniformly sampled across Eb/N0 range (-5 to 10 dB), but:
- High Eb/N0 regions have very few errors (most samples are error-free)
- Model doesn't learn error patterns effectively
- Low Eb/N0 regions have too many errors (overwhelming noise)

**Root Cause**:
- Uniform sampling doesn't focus on error-prone regions
- Model sees mostly error-free samples at high Eb/N0
- Model sees mostly noise at low Eb/N0

**Impact**:
- Model doesn't learn to correct errors effectively
- Poor generalization to medium Eb/N0 where errors are common

#### Issue 3: Loss Function and Training Strategy

**Problem**: 
- Bit-level loss function (BCE) doesn't consider codeword structure
- No emphasis on correcting actual errors vs. maintaining correct bits
- Training doesn't focus on challenging error patterns

**Root Cause**:
- Binary cross-entropy treats all bits equally
- Doesn't account for Hamming code structure
- No weighting of error-prone samples

**Impact**:
- Model may learn to be conservative (avoid changing bits)
- Doesn't learn to correct errors when they occur

### Analysis Report Summary

The analysis report (`data/analysis_report.md`) documented:

1. **Performance by Region**:
   - Low Eb/N0: 0.09% average improvement (slight)
   - Medium Eb/N0: -38.12% average (significant degradation)
   - High Eb/N0: -468.70% average (critical degradation)

2. **Key Issues Identified**:
   - Hard-decision input limitation
   - Training data distribution problems
   - Performance degradation at medium/high Eb/N0

3. **Recommendations**:
   - **Priority 1**: Implement soft-decision ML decoder (use LLRs)
   - **Priority 2**: Improve training strategy (weighted sampling, codeword-level loss)
   - **Priority 3**: Architecture improvements (deeper networks, alternative designs)

---

## Improvement Phases

Based on the analysis, we developed an improvement plan with five phases. Two phases have been completed, with three remaining.

### Phase 1 (Improvement): Analysis & Visualization âœ“ COMPLETE

**Status**: Successfully completed

**Deliverables**:
- `analyze_results.py`: Comprehensive analysis script
- `data/analysis_report.md`: Detailed performance analysis report
- `data/comparison_analysis.png`: Annotated comparison visualization
- Identified three key issues and provided recommendations

**Impact**: Provided clear understanding of performance problems and root causes.

---

### Phase 2 (Improvement): Soft-Decision ML Decoder âœ“ COMPLETE

**Objective**: Modify ML decoder to accept soft-decision inputs (LLRs) instead of hard bits.

**Implementation Details**:

1. **Modified `DirectMappingDecoder`** (`src/ml_decoder.py`):
   - Added `use_soft_input` parameter
   - When `True`, model expects Log-Likelihood Ratios (LLRs) instead of hard bits
   - LLRs are continuous values indicating bit reliability
   - Architecture remains the same (fully connected layers)

2. **Updated Training Data Generation**:
   - Modified `generate_training_data()` function
   - When `use_soft_input=True`:
     - Computes LLRs from noisy symbols: `LLR = 2 Ã— symbol / ÏƒÂ²`
     - Clips LLRs to reasonable range [-10, 10] for numerical stability
     - Uses LLRs as input features instead of hard bits

3. **Updated Evaluation**:
   - Modified `simulate_ml_decoder()` to use soft inputs when model expects them
   - Updated `run_ml_evaluation()` to support soft-decision models
   - Properly computes LLRs during evaluation

4. **New Training Script**:
   - Created `train_ml_decoder_soft.py` for training soft-decision models
   - Same interface as hard-decision training, but uses `use_soft_input=True`

**Key Code Changes**:
```python
# src/ml_decoder.py
class DirectMappingDecoder(nn.Module):
    def __init__(self, ..., use_soft_input=False):
        self.use_soft_input = use_soft_input
        # Architecture unchanged, but expects different input range

def generate_training_data(..., use_soft_input=False):
    if use_soft_input:
        # Compute LLRs
        noise_variance, _ = ebno_to_noise_variance(eb_no_db, rate)
        llrs = 2 * noisy_symbols / noise_variance
        llrs = np.clip(llrs, -10.0, 10.0)
        inputs.append(llrs.astype(np.float32))
    else:
        # Use hard bits
        rx_bits = bpsk_demodulate_hard(noisy_symbols)
        inputs.append(rx_bits.astype(np.float32))
```

**Training Process**:
```bash
python train_ml_decoder_soft.py \
    --approach direct \
    --num_samples 100000 \
    --epochs 50 \
    --output models/ml_decoder_direct_soft.pth
```

**Results**: **MAJOR SUCCESS** ðŸŽ‰

The soft-decision ML decoder showed **dramatic improvement**:

1. **Training Metrics**:
   - Validation accuracy: **83.42%** (vs. 78.00% for hard-decision)
   - Validation loss: **0.1473** (vs. 0.2287 for hard-decision)
   - **35.59% improvement** in validation loss

2. **BER Performance**:
   - **Average vs. classical**: **-43.3%** (better!) vs. +116.7% (worse) for hard-decision
   - **Medium Eb/N0 (0-5 dB)**: **-44.4%** (better!) vs. +38.1% (worse) for hard-decision
   - **Transformation**: From 116.7% worse to 43.3% better = **160 percentage point improvement**

3. **Performance by Region**:
   - **Low Eb/N0**: 11-22% better than classical (vs. 0-3% for hard-decision)
   - **Medium Eb/N0**: 25-67% better than classical (vs. 6-101% worse for hard-decision)
   - **High Eb/N0**: Achieves zero errors at 8-10 dB (vs. still having errors for hard-decision)

4. **Key Examples**:
   - At 0 dB: 25.7% better (vs. 6.4% worse)
   - At 2 dB: 38.8% better (vs. 20.4% worse)
   - At 4 dB: 58.4% better (vs. 56.0% worse)
   - At 5 dB: 67.0% better (vs. 101.5% worse)

**Why Soft-Decision Works**:

1. **Reliability Information**: LLRs provide confidence information
   - Large positive LLR â†’ very confident bit is 0
   - Large negative LLR â†’ very confident bit is 1
   - Small LLR â†’ uncertain bit
   - Model can learn to weight uncertain bits differently

2. **Continuous Values**: More information than binary 0/1
   - Model can learn smooth functions
   - Better gradient flow during training
   - More expressive than hard decisions

3. **Better Learning**: Model learns to leverage channel information
   - Can identify and focus on uncertain bits
   - Can make better decisions when multiple bits are uncertain
   - Learns error patterns more effectively

**Documentation**: 
- Created `docs/SOFT_DECISION_ANALYSIS.md` with detailed analysis
- Generated comparison plots: `data/comparison_all_decoders.png`
- Training comparison: `data/training_comparison_soft_vs_hard.png`

**Status**: âœ… **COMPLETE AND SUCCESSFUL**

---

### Phase 3 (Improvement): Improved Training Strategy âœ“ COMPLETE

**Objective**: Improve training data distribution and loss function to better learn error patterns.

**Status**: Successfully completed

**Implementation Details**:

1. **Weighted Training Data Sampling**:
   - Modified `generate_training_data()` to support weighted sampling
   - 70% of training samples focused on error-prone region (0-5 dB by default)
   - 30% of samples from full Eb/N0 range for generalization
   - Configurable focus region via `focus_ebno_range` parameter
   - Better learning of error patterns in critical regions

2. **Codeword-Level Loss Function**:
   - Created `CodewordLoss` class in `src/ml_decoder.py`
   - Combines bit-level BCE loss (50% weight) with codeword-level penalty (50% weight)
   - Penalizes incorrect codewords more than individual bit errors
   - Encourages model to learn Hamming code structure
   - Optional: can be enabled via `use_codeword_loss` flag in training

3. **Data Augmentation**:
   - Implemented `augment_training_data()` function
   - Adds controlled Gaussian noise to training samples
   - Configurable augmentation ratio (default: 10% of data)
   - Configurable noise level (default: 0.1)
   - Improves model robustness and generalization

**Key Code Changes**:
```python
# Weighted sampling in generate_training_data()
if weighted_sampling:
    if np.random.random() < 0.7:  # 70% from focus region
        eb_no_db = np.random.uniform(focus_min, focus_max)
    else:  # 30% from full range
        eb_no_db = np.random.uniform(ebno_min, ebno_max)

# Codeword-level loss
class CodewordLoss(nn.Module):
    def forward(self, outputs, targets):
        bit_loss = bce_loss(outputs, targets)
        codeword_penalty = (1.0 - codeword_correct).mean()
        return 0.5 * bit_loss + 0.5 * codeword_penalty

# Data augmentation
augmented_inputs, augmented_targets = augment_training_data(
    inputs, targets, noise_level=0.1, augmentation_ratio=0.1
)
```

**Training Script**:
- Created `train_ml_decoder_phase3.py` for Phase 3 training
- Combines all Phase 3 improvements:
  - Soft-decision inputs (from Phase 2)
  - Weighted sampling
  - Codeword loss (optional)
  - Data augmentation (optional)

**Usage**:
```bash
python train_ml_decoder_phase3.py \
    --approach direct \
    --num_samples 100000 \
    --epochs 50 \
    --use_codeword_loss \
    --augment_ratio 0.1 \
    --output models/ml_decoder_phase3.pth
```

**Expected Benefits**:
- Better learning of error patterns in critical regions
- Improved generalization
- Potentially 10-20% additional improvement over soft-decision

**Status**: âœ… **COMPLETE**

---

## Project Completion Status

**All improvement phases are now complete!**

The project has successfully progressed through:
- âœ… Phase 1-6 (Initial): Complete implementation
- âœ… Phase 1 (Improvement): Analysis & Visualization
- âœ… Phase 2 (Improvement): Soft-Decision ML Decoder
- âœ… Phase 3 (Improvement): Improved Training Strategy
- âœ… Phase 4 (Improvement): Architecture Improvements
- âœ… Phase 5 (Improvement): Final Evaluation & Documentation

The codebase now includes comprehensive ML decoder implementations with significant performance improvements over the classical baseline.

---

### Phase 4 (Improvement): Architecture Improvements âœ“ COMPLETE

**Objective**: Experiment with deeper/wider networks and alternative architectures.

**Status**: Successfully completed

**Implementation Details**:

1. **Deeper Networks**:
   - Created `DeepDirectMappingDecoder` class
   - Architecture: 7 â†’ 128 â†’ 64 â†’ 32 â†’ 16 â†’ 4 (5 layers vs. 3 in standard)
   - More layers for learning hierarchical error patterns
   - Better capacity for complex error correction

2. **Wider Networks**:
   - Created `WideDirectMappingDecoder` class
   - Architecture: 7 â†’ 256 â†’ 128 â†’ 4 (wider layers)
   - More neurons per layer for increased capacity
   - Better for learning complex patterns in each layer

3. **Residual Connections**:
   - Created `ResidualDirectMappingDecoder` class
   - Architecture: 7 â†’ 128 â†’ 64 â†’ 32 â†’ 4 with skip connections
   - Residual blocks help with gradient flow
   - Enables learning identity mappings and deeper training

**Key Code Changes**:
```python
# Deep architecture
class DeepDirectMappingDecoder(nn.Module):
    # 7 â†’ 128 â†’ 64 â†’ 32 â†’ 16 â†’ 4
    # 5 hidden layers for deeper learning

# Wide architecture  
class WideDirectMappingDecoder(nn.Module):
    # 7 â†’ 256 â†’ 128 â†’ 4
    # Wider layers for more capacity

# Residual architecture
class ResidualDirectMappingDecoder(nn.Module):
    # 7 â†’ 128 â†’ 64 â†’ 32 â†’ 4 with skip connections
    # Residual blocks for better gradient flow
```

**Training Script**:
- Created `train_ml_decoder_phase4.py` for Phase 4 training
- Supports architecture selection: `--architecture {standard,deep,wide,residual}`
- Combines all previous improvements:
  - Phase 2: Soft-decision inputs (LLRs)
  - Phase 3: Weighted sampling, codeword loss, data augmentation
  - Phase 4: Architecture improvements

**Usage**:
```bash
# Train deep architecture
python train_ml_decoder_phase4.py \
    --architecture deep \
    --num_samples 100000 \
    --epochs 50 \
    --use_codeword_loss \
    --output models/ml_decoder_phase4_deep.pth

# Train wide architecture
python train_ml_decoder_phase4.py \
    --architecture wide \
    --output models/ml_decoder_phase4_wide.pth

# Train residual architecture
python train_ml_decoder_phase4.py \
    --architecture residual \
    --output models/ml_decoder_phase4_residual.pth
```

**Architecture Comparison**:
- **Standard**: 7 â†’ 64 â†’ 32 â†’ 4 (baseline, ~4K parameters)
- **Deep**: 7 â†’ 128 â†’ 64 â†’ 32 â†’ 16 â†’ 4 (~20K parameters)
- **Wide**: 7 â†’ 256 â†’ 128 â†’ 4 (~50K parameters)
- **Residual**: 7 â†’ 128 â†’ 64 â†’ 32 â†’ 4 with skip connections (~30K parameters)

**Expected Benefits**:
- Better capacity for learning complex error patterns
- Potentially 5-15% additional improvement over Phase 3
- Different architectures may excel in different Eb/N0 regions

**Status**: âœ… **COMPLETE**

---

### Phase 5 (Improvement): Final Evaluation & Documentation âœ“ COMPLETE

**Objective**: Comprehensive evaluation of all improved models and final documentation.

**Status**: Successfully completed

**Implementation Details**:

1. **Comprehensive Evaluation Script**:
   - Created `final_evaluation_phase5.py` for complete evaluation
   - Evaluates all model variants:
     - Classical decoder (baseline)
     - Hard-decision ML decoder
     - Soft-decision ML decoder (Phase 2)
     - Phase 3 improved decoder
     - Phase 4 architecture variants (deep, wide, residual)
   - Automatic model detection and loading
   - Consistent evaluation across all models

2. **Comprehensive Comparison Plots**:
   - Single plot showing all model variants
   - Color-coded by phase
   - Annotated with phase information
   - High-resolution output (300 DPI)
   - Saved to `data/phase5_final_comparison.png`

3. **Performance Summary Report**:
   - Calculates average improvements vs. classical
   - Identifies best and worst performance points
   - Detailed BER table for all Eb/N0 values
   - Key findings and recommendations
   - Saved to `data/phase5_final_report.md`

4. **Results Storage**:
   - All results saved to `data/phase5_final_results.npy`
   - Includes raw BER data and summary statistics
   - Enables further analysis

**Key Features**:
```python
# Automatic model loading by type
def load_model_by_type(model_path, device='cpu'):
    # Detects architecture from filename
    # Loads appropriate model class
    # Returns model, use_soft_input, architecture

# Comprehensive evaluation
results = evaluate_all_models(ebno_range, num_bits_per_point, seed, device)

# Performance summary
summary = calculate_performance_summary(results)

# Final report generation
generate_final_report(results, summary)
```

**Usage**:
```bash
python final_evaluation_phase5.py
```

**Output Files**:
- `data/phase5_final_comparison.png`: Comprehensive comparison plot
- `data/phase5_final_report.md`: Performance summary report
- `data/phase5_final_results.npy`: Raw results data

**Expected Deliverables**:
- âœ… Final comparison plots
- âœ… Updated documentation
- âœ… Performance summary report
- âœ… Recommendations for future work

**Status**: âœ… **COMPLETE**

---

## Codebase Architecture

This section provides a detailed overview of how the codebase is organized and how components interact.

### Core Modules (`src/`)

#### 1. `hamming.py` - Hamming Code Implementation

**Purpose**: Implements Hamming(7,4) encoder.

**Key Functions**:
- `encode(data_bits)`: Encodes 4 data bits into 7-bit codeword
- Uses systematic generator matrix
- All 16 possible messages supported

**Usage**:
```python
from src.hamming import encode
data = np.array([1, 0, 1, 1], dtype=np.uint8)
codeword = encode(data)  # Returns 7-bit codeword
```

---

#### 2. `classical_decoder.py` - Classical Decoder

**Purpose**: Syndrome-based Hamming decoder.

**Key Classes**:
- `ClassicalDecoder`: Main decoder class

**Key Methods**:
- `decode(received_bits)`: Decodes 7 received bits
  - Returns: `(data_bits, corrected_codeword, error_info)`
  - Detects and corrects single-bit errors

**Usage**:
```python
from src.classical_decoder import ClassicalDecoder
decoder = ClassicalDecoder()
data, codeword, info = decoder.decode(received_bits)
```

---

#### 3. `channel.py` - Channel Simulation

**Purpose**: BPSK modulation, AWGN channel, and demodulation.

**Key Functions**:
- `bpsk_modulate(bits)`: Converts bits to BPSK symbols (0â†’+1, 1â†’-1)
- `awgn_channel(symbols, ebno_db, rate, seed)`: Adds AWGN noise
- `bpsk_demodulate_hard(noisy_symbols)`: Hard-decision demodulation
- `bpsk_demodulate_soft(noisy_symbols, noise_variance)`: Soft-decision (LLRs)
- `ebno_to_noise_variance(ebno_db, rate)`: Converts Eb/N0 to noise variance

**Usage**:
```python
from src.channel import bpsk_modulate, awgn_channel, bpsk_demodulate_hard
symbols = bpsk_modulate(codeword)
noisy = awgn_channel(symbols, ebno_db=5.0, rate=4/7)
bits = bpsk_demodulate_hard(noisy)
```

---

#### 4. `evaluation.py` - Evaluation Framework

**Purpose**: BER calculation and visualization for classical decoder.

**Key Functions**:
- `calculate_ber(decoder, ebno_db, num_bits, seed)`: Calculate BER at one Eb/N0
- `evaluate_decoder(decoder, ebno_range, num_bits_per_point, seed)`: Evaluate across range
- `plot_ber_curve(ebno_range, ber_values, title, save_path)`: Plot results

**Usage**:
```python
from src.evaluation import evaluate_decoder
ebno_range = np.linspace(-5, 10, 16)
ber_values = evaluate_decoder(decoder, ebno_range, num_bits=100000)
```

---

#### 5. `ml_decoder.py` - ML Model Implementation

**Purpose**: Neural network architectures and training for ML decoder.

**Key Classes**:
- `DirectMappingDecoder`: Neural network (7 â†’ hidden â†’ 4)
- `PostProcessingDecoder`: Neural network for post-processing
- `MLDecoder`: Wrapper class for inference
- `HammingDataset`: PyTorch dataset for training

**Key Functions**:
- `generate_training_data(...)`: Generate synthetic training data
- `train_model(...)`: Train neural network
- `save_model(...)`, `load_model(...)`: Model persistence

**Architecture Details**:
```python
DirectMappingDecoder:
  Input: 7 (received bits or LLRs)
  Hidden: [64, 32] (fully connected, ReLU, Dropout 0.2)
  Output: 4 (data bits, Sigmoid)
  
  Loss: Binary Cross-Entropy
  Optimizer: Adam (lr=0.001)
```

**Usage**:
```python
from src.ml_decoder import DirectMappingDecoder, MLDecoder, generate_training_data, train_model

# Generate data
inputs, targets = generate_training_data(100000, (-5, 10), use_soft_input=True)

# Create model
model = DirectMappingDecoder(use_soft_input=True)

# Train
train_losses, val_losses, val_accs = train_model(model, train_loader, val_loader, epochs=50)

# Inference
ml_decoder = MLDecoder(model, use_soft_input=True)
decoded = ml_decoder.decode(received_bits)
```

---

#### 6. `ml_evaluation.py` - ML Evaluation

**Purpose**: Evaluation framework for ML decoder.

**Key Functions**:
- `simulate_ml_decoder(...)`: Simulate ML decoder on one sample
- `run_ml_evaluation(...)`: Evaluate ML decoder across Eb/N0 range

**Usage**:
```python
from src.ml_evaluation import run_ml_evaluation
results = run_ml_evaluation(
    model_path='models/ml_decoder_direct_soft.pth',
    approach='direct',
    ebno_range=np.linspace(-5, 10, 16),
    num_bits=100000,
    use_soft_input=True
)
```

---

### Training Scripts

#### `train_ml_decoder.py` - Hard-Decision Training

Trains ML decoder with hard-decision inputs (binary bits).

**Usage**:
```bash
python train_ml_decoder.py \
    --approach direct \
    --num_samples 100000 \
    --epochs 50 \
    --output models/ml_decoder_direct.pth
```

---

#### `train_ml_decoder_soft.py` - Soft-Decision Training

Trains ML decoder with soft-decision inputs (LLRs).

**Usage**:
```bash
python train_ml_decoder_soft.py \
    --approach direct \
    --num_samples 100000 \
    --epochs 50 \
    --output models/ml_decoder_direct_soft.pth
```

---

### Analysis and Comparison Scripts

#### `analyze_results.py` - Performance Analysis

Comprehensive analysis of decoder performance:
- Calculates detailed metrics
- Generates annotated plots
- Creates analysis report

**Usage**:
```bash
python analyze_results.py
# Generates:
# - data/comparison_analysis.png
# - data/analysis_report.md
```

---

#### `compare_decoders.py` - Side-by-Side Comparison

Compares classical and ML decoders:
- Runs both evaluations
- Generates comparison plots
- Saves results

**Usage**:
```bash
python compare_decoders.py \
    --model_path models/ml_decoder_direct_soft.pth \
    --approach direct \
    --use_soft_input
```

---

#### `generate_final_results.py` - Final Results Generation

Generates comprehensive final results and plots.

**Usage**:
```bash
python generate_final_results.py \
    --model_path models/ml_decoder_direct_soft.pth \
    --approach direct \
    --use_soft_input
```

---

### Verification Scripts

#### `verify_phase*.py` - Phase Verification

Six verification scripts (one per initial phase):
- `verify_phase1.py`: Hamming encoder/decoder
- `verify_phase2.py`: Channel simulation
- `verify_phase3.py`: Baseline evaluation
- `verify_phase4.py`: ML model development
- `verify_phase5.py`: ML integration
- `verify_phase6.py`: Final comparison

Each script verifies that the phase is working correctly.

---

### Data Flow

**Complete Transmission and Decoding Pipeline**:

```
1. Data Generation
   â””â”€> Random 4-bit data

2. Encoding
   â””â”€> Hamming(7,4) encoder
   â””â”€> 7-bit codeword

3. Modulation
   â””â”€> BPSK modulation
   â””â”€> Symbols: 0â†’+1, 1â†’-1

4. Channel
   â””â”€> AWGN noise (variance from Eb/N0)
   â””â”€> Noisy symbols

5. Demodulation
   â”œâ”€> Hard-decision: threshold at 0 â†’ binary bits
   â””â”€> Soft-decision: compute LLRs â†’ continuous values

6. Decoding
   â”œâ”€> Classical: syndrome lookup â†’ 4-bit data
   â””â”€> ML: neural network â†’ 4-bit data

7. Evaluation
   â””â”€> Compare decoded data to original
   â””â”€> Calculate BER
```

---

## Key Technical Details

### Hamming(7,4) Code

**Code Parameters**:
- **Code Rate**: R = 4/7 (4 data bits â†’ 7 codeword bits)
- **Error Correction**: Single-bit errors
- **Minimum Distance**: d_min = 3

**Generator Matrix** (systematic form):
```
G = [Iâ‚„ | P]
    [1 0 0 0 | 1 1 0]
    [0 1 0 0 | 1 0 1]
    [0 0 1 0 | 0 1 1]
    [0 0 0 1 | 1 1 1]
```

**Parity Check Matrix**:
```
H = [Páµ€ | Iâ‚ƒ]
    [1 1 0 1 | 1 0 0]
    [1 0 1 1 | 0 1 0]
    [0 1 1 1 | 0 0 1]
```

**Syndrome Decoding**:
- Syndrome = H Ã— received_bitsáµ€
- Syndrome identifies error position
- Correct by flipping bit at error position

---

### Channel Model

**BPSK Modulation**:
- Binary 0 â†’ +1 (symbol)
- Binary 1 â†’ -1 (symbol)
- Energy per bit: E_b = 1

**AWGN Channel**:
- Noise: n ~ N(0, ÏƒÂ²)
- Received: y = x + n
- Noise variance: ÏƒÂ² = Nâ‚€/2 = 1/(2 Ã— R Ã— 10^(Eb/N0/10))

**Eb/N0 Conversion**:
- Eb/N0 (dB) = 10 Ã— logâ‚â‚€(E_b / Nâ‚€)
- Nâ‚€ = 2 Ã— ÏƒÂ² Ã— R
- For BPSK: E_b = 1, so ÏƒÂ² = 1/(2 Ã— R Ã— 10^(Eb/N0/10))

**Demodulation**:
- **Hard-decision**: y > 0 â†’ 0, y < 0 â†’ 1
- **Soft-decision (LLR)**: LLR = 2y/ÏƒÂ²
  - Positive LLR â†’ likely 0
  - Negative LLR â†’ likely 1
  - |LLR| indicates confidence

---

### ML Architecture Details

**DirectMappingDecoder**:
```
Input Layer:  7 neurons (received bits or LLRs)
Hidden Layer 1: 64 neurons (ReLU, Dropout 0.2)
Hidden Layer 2: 32 neurons (ReLU, Dropout 0.2)
Output Layer: 4 neurons (Sigmoid)
```

**Training**:
- Loss: Binary Cross-Entropy (BCE)
- Optimizer: Adam (learning rate = 0.001)
- Batch size: 64
- Epochs: 50 (default)
- Validation split: 80% train, 20% validation

**Inference**:
- Input: 7 received bits (hard) or LLRs (soft)
- Output: 4 probabilities (0-1)
- Threshold: 0.5 â†’ binary decision

---

### Performance Metrics

**Bit Error Rate (BER)**:
- BER = (number of bit errors) / (total bits transmitted)
- Evaluated across Eb/N0 range
- Typically plotted on log scale

**Improvement Metric**:
- Improvement % = ((BER_classical - BER_ML) / BER_classical) Ã— 100
- Positive = ML better, Negative = ML worse

**Validation Metrics**:
- **Accuracy**: Fraction of codewords decoded correctly
- **Loss**: Binary cross-entropy loss
- **Codeword-level**: All 4 bits must be correct

---

## Summary

This codebase represents a complete journey from initial implementation through performance analysis and improvement. The project demonstrates:

1. **Initial Development**: Six phases building a complete ML-assisted decoder system
2. **Problem Discovery**: Comprehensive analysis revealing performance issues
3. **Root Cause Analysis**: Identification of hard-decision limitation and training issues
4. **Successful Improvement**: Soft-decision ML decoder achieving 43.3% improvement over classical
5. **Future Work**: Three remaining improvement phases for further enhancement

The codebase is well-structured, thoroughly tested (66 unit tests), and fully documented. The soft-decision improvement represents a major success, transforming the ML decoder from underperforming to significantly outperforming the classical decoder.

---

## References

- Hamming, R. W. (1950). "Error detecting and error correcting codes". Bell System Technical Journal.
- Proakis, J. G., & Salehi, M. (2008). "Digital Communications" (5th ed.). McGraw-Hill.
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). "Deep Learning". MIT Press.

---

**Last Updated**: December 2025  
**Status**: Phases 1-6 (Initial) âœ“ Complete | Phase 1-5 (Improvement) âœ“ Complete | **PROJECT COMPLETE** âœ…

